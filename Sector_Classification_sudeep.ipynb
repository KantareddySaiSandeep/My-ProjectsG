{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.twitter import Twitter\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "pd.set_option('max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "base_path = r'C:/Users/sshouche/Desktop/Tweet Classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"One time processing to get sector tagged data\"\"\"\n",
    "\n",
    "# drop_cols = ['Companies', 'Locations', 'Themes', 'Tweethistoryid']\n",
    "\n",
    "# for month in ['Apr', 'May', 'Jun', 'Jul', 'Aug']:\n",
    "#     dfs = pd.read_excel(base_path+r'TweetDump_last6months/TweetsDump_'+str(month)+'.xlsx', sheet_name=None)\n",
    "\n",
    "#     df = pd.concat(dfs, ignore_index=True)\n",
    "#     df = df.drop(columns=drop_cols, axis=1)\n",
    "#     df = df.dropna()\n",
    "\n",
    "#     df.to_json(base_path+r'TweetDump_last6months/Tweets_'+str(month)+'2019.json')\n",
    "\n",
    "#     del df, dfs\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"One time processing to combine sector tagged data\"\"\"\n",
    "\n",
    "# for month in ['Apr', 'May', 'Jun', 'Jul', 'Aug']:\n",
    "#     df = pd.read_json(base_path+r'TweetDump_last6months/Tweets_'+str(month)+'2019.json')\n",
    "#     if month=='Apr':\n",
    "#         df_combo = df\n",
    "#     else:\n",
    "#         df_combo = pd.concat([df_combo, df], axis=0)\n",
    "\n",
    "# del df\n",
    "# gc.collect()\n",
    "\n",
    "# df_combo = df_combo.reset_index()\n",
    "# df_combo['Sectors'] = df_combo['Sectors'].apply(lambda x: [str(y).strip() for y in str(x).split(',')])\n",
    "# df_combo.to_json(base_path+r'TweetDump_last6months/Tweets_Combo.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## DATA JOINING COMPLETE ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data and Helper Functions\"\"\"\n",
    "freqdist = nltk.FreqDist()\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "new_words = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\",\"eight\",\"nine\", \"zero\", \n",
    "             \"ten\", \"twenty\", \"thirty\", \"fourty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninty\", \n",
    "             \"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\", \"ninth\", \"tenth\",\n",
    "             \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\", \n",
    "             \"hundreds\", \"thousands\", \"millions\", \"billions\", \"trillions\",\n",
    "             \"world\", \"today\", \"would\", \"could\", \"future\", \"people\", \n",
    "             '...', 'via', 'see', 'new', 'end', 'amp', \n",
    "             'like', 'time', 'need', 'know', 'ever']\n",
    "stop_words = list(stop_words.union(new_words))\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=True, strip_handles=True, reduce_len=True)\n",
    "\n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper Functions\"\"\"\n",
    "def get_links(tweet):\n",
    "    reg_ex = r'((?:(?:https?|ftp|file):\\/\\/|www\\.|ftp\\.)*(?:[-a-zA-Z0-9@:%_\\+~.#=]{2,256})?([-a-zA-Z0-9@:%_\\+~#=]*)\\.[a-z]{2,6}\\b(?:[-a-zA-Z0-9@:%_\\+.~#?&\\/\\/=]*)*)'\n",
    "    link_regex = re.compile(reg_ex, re.DOTALL)\n",
    "    links = re.findall(link_regex, tweet)\n",
    "    link_list = []\n",
    "    for link in links:\n",
    "        link_list.append(link[0])    \n",
    "    return link_list\n",
    "\n",
    "def get_tickers(tweet):\n",
    "    ticker_regex = re.compile(r'\\$\\w*', re.DOTALL)\n",
    "    tickers = re.findall(ticker_regex, tweet)\n",
    "    ticker_list = []\n",
    "    for ticker in tickers:\n",
    "        ticker_list.append(ticker[0])    \n",
    "    return ticker_list\n",
    "\n",
    "def get_special(tweet, special_prefixes=['@', '#']):\n",
    "    words_list = []\n",
    "    for word in tweet.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] in special_prefixes:\n",
    "                words_list.append(word)\n",
    "    return words_list\n",
    "\n",
    "def strip_links(tweet):\n",
    "    # reg_ex = r'((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)'\n",
    "    # ((\\b)?((https|https|ftp|file):\\/\\/)|(www\\.|ftp\\.))?((?:[-a-zA-Z0-9@:%_\\+~.#=]{2,256}\\.)?([-a-zA-Z0-9@:%_\\+~#=]*)\\.[a-z]{2,6}\\b(?:[-a-zA-Z0-9@:%_\\+.~#?&\\/\\/=]*))*(\\b)?\n",
    "    reg_ex = r'((?:(?:https?|ftp|file):\\/\\/|www\\.|ftp\\.)*(?:[-a-zA-Z0-9@:%_\\+~.#=]{2,256})?([-a-zA-Z0-9@:%_\\+~#=]*)\\.[a-z]{2,6}\\b(?:[-a-zA-Z0-9@:%_\\+.~#?&\\/\\/=]*)*)'\n",
    "    tweet = re.sub(reg_ex, ' ', tweet)\n",
    "    # link_regex = re.compile(reg_ex, re.DOTALL)\n",
    "    # links = re.findall(link_regex, tweet)\n",
    "    # print (links)\n",
    "    # for link in links:\n",
    "    #     tweet = tweet.replace(link[0], ' ')    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "def strip_special(tweet, special_prefixes=['@', '#']):\n",
    "    for separator in string.punctuation:\n",
    "        if separator not in special_prefixes:\n",
    "            tweet = tweet.replace(separator,' ')\n",
    "    words_list = []\n",
    "    for word in tweet.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in special_prefixes:\n",
    "                words_list.append(word)\n",
    "    return ' '.join(words_list)\n",
    "\n",
    "\n",
    "def clean_tweets(tweet, keep_list=[]):\n",
    "    tweet = re.sub(r\"(Dr\\.)\", \"Doctor \", tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', ' ', tweet) # remove old style retweet text \"RT\"\n",
    "    \n",
    "    if '$' not in keep_list:\n",
    "        tweet = re.sub(r'\\$\\w*', ' ', tweet)     # remove stock market tickers like $GE\n",
    "    \n",
    "    tweet = re.sub(r'(?:\\.?)([\\w\\-_+#~!$&\\'\\.]+(?<!\\.)(@|[ ]?\\(?[ ]?(at|AT)[ ]?\\)?[ ]?)\\\n",
    "                   (?<!\\.)[\\w]+[\\w\\-\\.]*\\.[a-zA-Z-]{2,3})(?:[^\\w])', ' ', tweet) # remove emails\n",
    "    tweet = strip_links(tweet) # remove links\n",
    "    \n",
    "    if '@' not in keep_list:\n",
    "        tweet = re.sub(r'\\@[\\w.]*', ' ', tweet) #remove mentions\n",
    "    \n",
    "    if '#' not in keep_list:\n",
    "        tweet = re.sub(r'\\#[\\w.]*', ' ', tweet) # remove # from the hashtags\n",
    "        # tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    tweet = re.sub(r'([\\d]+)([\\.]{1}[\\d]*)*', ' ', tweet) # remove numbers\n",
    "    tweet = re.sub(r'([\\d]+)([\\/]{1}[\\d]+)', ' ', tweet) # remove fractions\n",
    "    tweet = re.sub(r'([\\d+])', ' ', tweet) # remove integers\n",
    "    \n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"what’s\", \"what is \", tweet)\n",
    "    tweet = re.sub(r\"\\'s\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\'ve\", \" have \", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"can not \", tweet)\n",
    "    tweet = re.sub(r\"n't\", \" not \", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"i am \", tweet)\n",
    "    tweet = re.sub(r\"\\'re\", \" are \", tweet)\n",
    "    tweet = re.sub(r\"\\'d\", \" would \", tweet)\n",
    "    tweet = re.sub(r\"\\'ll\", \" will \", tweet)\n",
    "    tweet = re.sub(r\"\\'scuse\", \" excuse \", tweet)\n",
    "    tweet = re.sub(\"\\W\", \" \", tweet) # remove single char words\n",
    "    tweet = re.sub(\"\\s+\", \" \", tweet) # remove continuous spaces\n",
    "    tweet = tweet.strip(\" \")\n",
    "    tweet = unicodedata.normalize('NFKD', tweet).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # symbol left after removing mentions, hashtags, links, emails, etc.\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'/', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    #remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    tweet = re.sub(r'_',' ', tweet)\n",
    "    tweet = tweet.strip(\" \")\n",
    "    tweet = ' '.join([x for x in tweet.split()])\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "def process_tweets(tweet, lemma_stem=True):\n",
    "    tweet = re.sub(r'\\#', '', tweet)\n",
    "    \n",
    "    tweet = [x for x in tweet.split() if len(x)>1]\n",
    "    tweet = [x for x in tweet if x not in stop_words]\n",
    "    tweet = [x for x in tweet if x not in emoticons]\n",
    "    tweet = [x for x in tweet if x not in string.punctuation]\n",
    "\n",
    "    tweet = ' '.join(tweet)\n",
    "    \n",
    "    # tokenize tweets\n",
    "    tweet_tokens = tweet_tokenizer.tokenize(tweet)\n",
    "\n",
    "    bigrams = nltk.bigrams(tweet_tokens)\n",
    "    bigrams = ['_'.join(x) for x in bigrams]\n",
    "\n",
    "    tweet_tokens = list(tweet_tokens+bigrams)\n",
    "    \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if lemma_stem:\n",
    "            lemmatize_word = lemmatizer.lemmatize(word)\n",
    "            stem_word = stemmer.stem(lemmatize_word) # stemming word\n",
    "            freqdist[stem_word]+=1\n",
    "            tweets_clean.append(stem_word)\n",
    "        else:\n",
    "            freqdist[word]+=1\n",
    "            tweets_clean.append(word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(base_path+r'TweetDump_last6months/Tweets_Combo.json')\n",
    "# df['isTagged']=0\n",
    "# df['sectorTags']=''\n",
    "# df['verticalTags']=''\n",
    "\n",
    "# df = df[pd.notnull(df['TweetFulltext'])]\n",
    "# df.to_json(base_path+r'TweetDump_last6months/Tweets_Sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Load Combined & Tagged data\"\"\"\n",
    "df_combo = pd.read_json(base_path+r'TweetDump_last6months/Tweets_Combo.json')\n",
    "# df_combo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>TweetFulltext</th>\n",
       "      <th>Sectors</th>\n",
       "      <th>Links</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Tickers</th>\n",
       "      <th>CleanText</th>\n",
       "      <th>ProcessedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>874696</td>\n",
       "      <td>NealSchaffer</td>\n",
       "      <td>‘Do brands ever check on the welfare of influencers?’: YouTube stars confront mental health issues https://t.co/CIS2lnSUI1 #influencermarketing https://t.co/JfHmY8ZrVp</td>\n",
       "      <td>[Advertising, Application Software]</td>\n",
       "      <td>[https://t.co/CIS2lnSUI1, https://t.co/JfHmY8ZrVp]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#influencermarketing]</td>\n",
       "      <td>[]</td>\n",
       "      <td>do brands ever check on the welfare of influencers youtube stars confront mental health issues influencermarketing</td>\n",
       "      <td>[brands, check, welfare, influencers, youtube, stars, confront, mental, health, issues, influencermarketing, brands_check, check_welfare, welfare_influencers, influencers_youtube, youtube_stars, stars_confront, confront_mental, mental_health, health_issues, issues_influencermarketing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100006</td>\n",
       "      <td>874702</td>\n",
       "      <td>NealSchaffer</td>\n",
       "      <td>Microinfluencers do endorsements right, and here are six awesome examples. https://t.co/l8RDTmWuj1 #influencermarketing https://t.co/4XAREMPD5G</td>\n",
       "      <td>[Advertising]</td>\n",
       "      <td>[https://t.co/l8RDTmWuj1, https://t.co/4XAREMPD5G]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#influencermarketing]</td>\n",
       "      <td>[]</td>\n",
       "      <td>microinfluencers do endorsements right and here are six awesome examples influencermarketing</td>\n",
       "      <td>[microinfluencers, endorsements, right, awesome, examples, influencermarketing, microinfluencers_endorsements, endorsements_right, right_awesome, awesome_examples, examples_influencermarketing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100088</td>\n",
       "      <td>874784</td>\n",
       "      <td>NealSchaffer</td>\n",
       "      <td>Six Tips for Using Promoted Tweets for Your Ecommerce Marketing Strategy via @ryankhgb #twitter #marketing https://t.co/B4lKQFiZVs https://t.co/n1tOJf2Qjb</td>\n",
       "      <td>[Advertising, Ecommerce]</td>\n",
       "      <td>[https://t.co/B4lKQFiZVs, https://t.co/n1tOJf2Qjb]</td>\n",
       "      <td>[@ryankhgb]</td>\n",
       "      <td>[#twitter, #marketing]</td>\n",
       "      <td>[]</td>\n",
       "      <td>six tips for using promoted tweets for your ecommerce marketing strategy via ryankhgb twitter marketing</td>\n",
       "      <td>[tips, using, promoted, tweets, ecommerce, marketing, strategy, ryankhgb, twitter, marketing, tips_using, using_promoted, promoted_tweets, tweets_ecommerce, ecommerce_marketing, marketing_strategy, strategy_ryankhgb, ryankhgb_twitter, twitter_marketing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1003910</td>\n",
       "      <td>1778605</td>\n",
       "      <td>jetcitystar</td>\n",
       "      <td>#NasaSocial #NasaSocial #NASA747 #NASAAmes #NASAArmstrong #SOFIAtelescope #NASA #AdrianaSays #CorporateCode #USAF #Veterans #JAGNV #MoffetField #bluecube #HeartMathCoach #johnmaxwellcoach #familia #colegas #entrenamiento #mettingplanners #speaker #speakers #espanol https://t.co/m7L3RjFZ6t</td>\n",
       "      <td>[Aerospace]</td>\n",
       "      <td>[https://t.co/m7L3RjFZ6t]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#NasaSocial, #NasaSocial, #NASA747, #NASAAmes, #NASAArmstrong, #SOFIAtelescope, #NASA, #AdrianaSays, #CorporateCode, #USAF, #Veterans, #JAGNV, #MoffetField, #bluecube, #HeartMathCoach, #johnmaxwellcoach, #familia, #colegas, #entrenamiento, #mettingplanners, #speaker, #speakers, #espanol]</td>\n",
       "      <td>[]</td>\n",
       "      <td>nasasocial nasasocial nasa nasaames nasaarmstrong sofiatelescope nasa adrianasays corporatecode usaf veterans jagnv moffetfield bluecube heartmathcoach johnmaxwellcoach familia colegas entrenamiento mettingplanners speaker speakers espanol</td>\n",
       "      <td>[nasasocial, nasasocial, nasa, nasaames, nasaarmstrong, sofiatelescope, nasa, adrianasays, corporatecode, usaf, veterans, jagnv, moffetfield, bluecube, heartmathcoach, johnmaxwellcoach, familia, colegas, entrenamiento, mettingplanners, speaker, speakers, espanol, nasasocial_nasasocial, nasasocial_nasa, nasa_nasaames, nasaames_nasaarmstrong, nasaarmstrong_sofiatelescope, sofiatelescope_nasa, nasa_adrianasays, adrianasays_corporatecode, corporatecode_usaf, usaf_veterans, veterans_jagnv, jagnv_moffetfield, moffetfield_bluecube, bluecube_heartmathcoach, heartmathcoach_johnmaxwellcoach, johnmaxwellcoach_familia, familia_colegas, colegas_entrenamiento, entrenamiento_mettingplanners, mettingplanners_speaker, speaker_speakers, speakers_espanol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1018644</td>\n",
       "      <td>1793339</td>\n",
       "      <td>pinstripedline</td>\n",
       "      <td>RT @RoyalNavy: The last Royal Navy helicopter to fly on maritime security operations in Oman has returned to the UK after a ten-year missio…</td>\n",
       "      <td>[Maritime]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@RoyalNavy]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>royalnavy the last royal navy helicopter to fly on maritime security operations in oman has returned to the uk after a ten year missio</td>\n",
       "      <td>[royalnavy, last, royal, navy, helicopter, fly, maritime, security, operations, oman, returned, uk, year, missio, royalnavy_last, last_royal, royal_navy, navy_helicopter, helicopter_fly, fly_maritime, maritime_security, security_operations, operations_oman, oman_returned, returned_uk, uk_year, year_missio]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index       id      ScreenName                                                                                                                                                                                                                                                                                      TweetFulltext                              Sectors                                               Links      Mentions                                                                                                                                                                                                                                                                                           Hashtags Tickers                                                                                                                                                                                                                                        CleanText                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ProcessedText\n",
       "0     100000   874696   NealSchaffer    ‘Do brands ever check on the welfare of influencers?’: YouTube stars confront mental health issues https://t.co/CIS2lnSUI1 #influencermarketing https://t.co/JfHmY8ZrVp                                                                                                                            [Advertising, Application Software]  [https://t.co/CIS2lnSUI1, https://t.co/JfHmY8ZrVp]  []            [#influencermarketing]                                                                                                                                                                                                                                                                             []      do brands ever check on the welfare of influencers youtube stars confront mental health issues influencermarketing                                                                                                                               [brands, check, welfare, influencers, youtube, stars, confront, mental, health, issues, influencermarketing, brands_check, check_welfare, welfare_influencers, influencers_youtube, youtube_stars, stars_confront, confront_mental, mental_health, health_issues, issues_influencermarketing]                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "1     100006   874702   NealSchaffer    Microinfluencers do endorsements right, and here are six awesome examples. https://t.co/l8RDTmWuj1 #influencermarketing https://t.co/4XAREMPD5G                                                                                                                                                    [Advertising]                        [https://t.co/l8RDTmWuj1, https://t.co/4XAREMPD5G]  []            [#influencermarketing]                                                                                                                                                                                                                                                                             []      microinfluencers do endorsements right and here are six awesome examples influencermarketing                                                                                                                                                     [microinfluencers, endorsements, right, awesome, examples, influencermarketing, microinfluencers_endorsements, endorsements_right, right_awesome, awesome_examples, examples_influencermarketing]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "10    100088   874784   NealSchaffer    Six Tips for Using Promoted Tweets for Your Ecommerce Marketing Strategy via @ryankhgb #twitter #marketing https://t.co/B4lKQFiZVs https://t.co/n1tOJf2Qjb                                                                                                                                         [Advertising, Ecommerce]             [https://t.co/B4lKQFiZVs, https://t.co/n1tOJf2Qjb]  [@ryankhgb]   [#twitter, #marketing]                                                                                                                                                                                                                                                                             []      six tips for using promoted tweets for your ecommerce marketing strategy via ryankhgb twitter marketing                                                                                                                                          [tips, using, promoted, tweets, ecommerce, marketing, strategy, ryankhgb, twitter, marketing, tips_using, using_promoted, promoted_tweets, tweets_ecommerce, ecommerce_marketing, marketing_strategy, strategy_ryankhgb, ryankhgb_twitter, twitter_marketing]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "100   1003910  1778605  jetcitystar     #NasaSocial #NasaSocial #NASA747 #NASAAmes #NASAArmstrong #SOFIAtelescope #NASA #AdrianaSays #CorporateCode #USAF #Veterans #JAGNV #MoffetField #bluecube #HeartMathCoach #johnmaxwellcoach #familia #colegas #entrenamiento #mettingplanners #speaker #speakers #espanol https://t.co/m7L3RjFZ6t  [Aerospace]                          [https://t.co/m7L3RjFZ6t]                           []            [#NasaSocial, #NasaSocial, #NASA747, #NASAAmes, #NASAArmstrong, #SOFIAtelescope, #NASA, #AdrianaSays, #CorporateCode, #USAF, #Veterans, #JAGNV, #MoffetField, #bluecube, #HeartMathCoach, #johnmaxwellcoach, #familia, #colegas, #entrenamiento, #mettingplanners, #speaker, #speakers, #espanol]  []      nasasocial nasasocial nasa nasaames nasaarmstrong sofiatelescope nasa adrianasays corporatecode usaf veterans jagnv moffetfield bluecube heartmathcoach johnmaxwellcoach familia colegas entrenamiento mettingplanners speaker speakers espanol  [nasasocial, nasasocial, nasa, nasaames, nasaarmstrong, sofiatelescope, nasa, adrianasays, corporatecode, usaf, veterans, jagnv, moffetfield, bluecube, heartmathcoach, johnmaxwellcoach, familia, colegas, entrenamiento, mettingplanners, speaker, speakers, espanol, nasasocial_nasasocial, nasasocial_nasa, nasa_nasaames, nasaames_nasaarmstrong, nasaarmstrong_sofiatelescope, sofiatelescope_nasa, nasa_adrianasays, adrianasays_corporatecode, corporatecode_usaf, usaf_veterans, veterans_jagnv, jagnv_moffetfield, moffetfield_bluecube, bluecube_heartmathcoach, heartmathcoach_johnmaxwellcoach, johnmaxwellcoach_familia, familia_colegas, colegas_entrenamiento, entrenamiento_mettingplanners, mettingplanners_speaker, speaker_speakers, speakers_espanol]\n",
       "1000  1018644  1793339  pinstripedline  RT @RoyalNavy: The last Royal Navy helicopter to fly on maritime security operations in Oman has returned to the UK after a ten-year missio…                                                                                                                                                       [Maritime]                           []                                                  [@RoyalNavy]  []                                                                                                                                                                                                                                                                                                 []      royalnavy the last royal navy helicopter to fly on maritime security operations in oman has returned to the uk after a ten year missio                                                                                                           [royalnavy, last, royal, navy, helicopter, fly, maritime, security, operations, oman, returned, uk, year, missio, royalnavy_last, last_royal, royal_navy, navy_helicopter, helicopter_fly, fly_maritime, maritime_security, security_operations, operations_oman, oman_returned, returned_uk, uk_year, year_missio]                                                                                                                                                                                                                                                                                                                                                                                                                                                       "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Collect links, mentions, hashtags and tickers, and process Tweets\"\"\"\n",
    "gc.collect()\n",
    "\n",
    "# Get links, mentions, hashtags, tickers\n",
    "# df_combo['Links'] = df_combo['TweetFulltext'].apply(lambda x: get_links(str(x)))\n",
    "# df_combo['Mentions'] = df_combo['TweetFulltext'].apply(lambda x: [str(y).replace(':', '') for y in get_special(str(x), ['@'])])\n",
    "# df_combo['Hashtags'] = df_combo['TweetFulltext'].apply(lambda x: get_special(str(x), ['#']))\n",
    "# df_combo['Tickers'] = df_combo['TweetFulltext'].apply(lambda x: get_tickers(str(x)))\n",
    "\n",
    "# Process tweet\n",
    "df_combo['CleanText'] = df_combo['TweetFulltext'].apply(lambda x: clean_tweets(str(x), keep_list=['#', '@', '$']))\n",
    "df_combo['ProcessedText'] = df_combo['CleanText'].apply(lambda x: process_tweets(str(x), lemma_stem=False))\n",
    "\n",
    "# Process tweet\n",
    "# df_combo['ProcessedText_ls'] = df_combo['CleanText'].apply(lambda x: process_tweets(str(x)))\n",
    "\n",
    "df_combo.to_json(base_path+r'TweetDump_last6months/Tweets_Input.json')\n",
    "df_combo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brands</td>\n",
       "      <td>1653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>check</td>\n",
       "      <td>6252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>welfare</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>influencers</td>\n",
       "      <td>1654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>youtube</td>\n",
       "      <td>2617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Keyword  Frequency\n",
       "0  brands       1653     \n",
       "1  check        6252     \n",
       "2  welfare      42       \n",
       "3  influencers  1654     \n",
       "4  youtube      2617     "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"\"\"Get keyword frequency distribution\"\"\"\n",
    "df_dist = pd.DataFrame.from_dict(freqdist, orient='index')\n",
    "df_dist = df_dist.reset_index()\n",
    "df_dist.columns = ['Keyword', 'Frequency']\n",
    "df_dist = df_dist[df_dist['Frequency']>10]\n",
    "df_dist.to_csv(base_path+r'TweetDump_last6months/Tweets_Keywords_no_lemma_stem.csv')\n",
    "df_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## DATA CLEANUP COMPLETE ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data Model\"\"\"\n",
    "import fasttext\n",
    "\n",
    "N_TAGS = 3\n",
    "\n",
    "# df_combo = pd.read_json(base_path+r'TweetDump_last6months/Tweets_Input.json')\n",
    "\n",
    "df_combo['Labels'] = df_combo['Sectors'].apply(lambda x: ' '.join(['__label__' + y.replace(' ', '_') for y in x]))\n",
    "\n",
    "df_combo['InTextData'] = df_combo['Labels'] + ' ' + df_combo['ProcessedText'].apply(lambda x: ' '.join(x))\n",
    "df_combo['InCleanData'] = df_combo['Labels'] + ' ' + df_combo['CleanText']\n",
    "\n",
    "df_combo[['InTextData']].dropna().to_csv(base_path+r'InTextData.txt', header=None, index=None, sep=' ')\n",
    "df_combo[['InCleanData']].dropna().to_csv(base_path+r'InCleanData.txt', header=None, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_model = fasttext.train_supervised(base_path+r'InCleanData.txt', lr=0.05, dim=500, ws=5, epoch=5, word_ngrams=2, loss='softmax', verbose=0)\n",
    "clean_model.save_model(base_path+r'clean_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_model = fasttext.train_supervised(base_path+r'InTextData.txt', lr=0.05, dim=100, ws=5, epoch=5, loss='softmax', verbose=0)\n",
    "# text_model.save_model(base_path+r'text_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashtag_model = fasttext.train_supervised(base_path+r'InHashtagData.txt', lr=0.05, dim=100, ws=5, epoch=5, loss='softmax', verbose=0)\n",
    "# hashtag_model.save_model(base_path+r'hashtag_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_clean = '#AI & #machinelearning let us discover solutions in a faster and more agile way than ever before:'\n",
    "in_clean = clean_tweets(str(in_clean), keep_list=['#'])\n",
    "print (in_clean)\n",
    "clean_result = clean_model.predict([in_clean], N_TAGS)\n",
    "print (clean_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combo.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Model training\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "df_combo = pd.read_json(base_path+r'TweetDump_last6months/Tweets_Input.json')\n",
    "# df_combo_2 = df_combo.join(pd.DataFrame(mlb.fit_transform(df_combo['Sectors']), columns=mlb.classes_, index=df_combo.index))\n",
    "\n",
    "y = mlb.fit_transform(df_combo.pop('Sectors'))\n",
    "X = df_combo['CleanText']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "clf = OneVsRestClassifier(MultinomialNB())\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "print (accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df['Sectors'].shape[0])\n",
    "print (df['Sectors'].isna().sum())\n",
    "print (df['Sectors'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x=list(df['Themes'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Locations','Companies','Sectors','Tweethistoryid','ScreenName'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Themes'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# le.fit(df['Themes'])\n",
    "# le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "# print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=['3D Printing', '5G', 'Artificial Intelligence', 'Cloud', 'Digital Assistants', 'E-commerce', 'Internet of Things', 'Mobile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance\n",
    "ADS\n",
    "Automotive\n",
    "Banking_Payments\n",
    "Construction\n",
    "Consumer\n",
    "Food_Service\n",
    "Medical_Devices\n",
    "Mining\n",
    "Packaging\n",
    "Pharma\n",
    "Power\n",
    "Retail\n",
    "Technology\n",
    "Travel_Tourism\n",
    "Upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sector']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['TweetFulltext'].str.contains('|'.join(Upstream))==True,'Sector']='Upstream'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Apr_Training_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.read_excel(r'C:\\Users\\kasandeep\\Desktop\\Projects\\Twitter_Sector_Classification\\Sector View Keywords\\Consolidated IC and Sector Screeners & Keywords.xlsx',sheet_name=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(df3['Screener/Filter word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[x.strip(\"#|@|'|\\xa0|\\u200f\") for x in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[x.replace(\" \",\"\") for x in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=set([x.replace(\"-\",\"\") for x in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=set([x.replace(\"'\",\"\") for x in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=set([x.lower() for x in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=[]\n",
    "# from nltk.stem import PorterStemmer\n",
    "# stemmer= PorterStemmer()\n",
    "# for i in x:\n",
    "#     y.append(stemmer.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=[]\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for i in x:\n",
    "    z.append(wordnet_lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\kasandeep\\Desktop\\Projects\\Twitter_Sector_Classification\\Sector View Keywords\\Travel_Tourism_words.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.write('\\n'.join(str(line) for line in set(Travel_Tourism)))\n",
    "    myfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance\n",
    "ADS\n",
    "Automotive\n",
    "Banking_Payments\n",
    "Construction\n",
    "Consumer\n",
    "Food_Service\n",
    "Medical_Devices\n",
    "Mining\n",
    "Packaging\n",
    "Pharma\n",
    "Power\n",
    "Retail\n",
    "Technology\n",
    "Travel_Tourism\n",
    "Upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r\"C:\\Users\\kasandeep\\Desktop\\Projects\\Twitter_Sector_Classification\\Sector View Keywords\\Upstream_words.txt\", \"r\",encoding=\"utf8\")\n",
    "z=Upstream\n",
    "for x in f:\n",
    "      z.append(x.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Upstream=[]\n",
    "for i in set(z):\n",
    "    if sum(i in s for s in df['TweetFulltext'])>50:\n",
    "        Upstream.append(i)\n",
    "        print(i,sum(i in s for s in df['TweetFulltext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r\"C:\\Users\\kasandeep\\Desktop\\Projects\\Twitter_Sector_Classification\\Sector View Keywords\\Packaging.txt\", \"r\",encoding=\"utf8\")\n",
    "z=[]\n",
    "for x in f:\n",
    "      z.append(x.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Consumer=[]\n",
    "for i in set(z):\n",
    "    if sum(i in s for s in df['TweetFulltext'])>50:\n",
    "        Consumer.append(i)\n",
    "        print(i,sum(i in s for s in df['TweetFulltext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "from gensim.models import CoherenceModel\n",
    "#np.random.seed(2018)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = df['TweetFulltext'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(bow_corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, dictionary=dictionary, texts=processed_docs, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize t# Visualize the topics\n",
    "# import pyLDAvis.gensim\n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        #model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model=gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=bow_corpus, texts=processed_docs, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
