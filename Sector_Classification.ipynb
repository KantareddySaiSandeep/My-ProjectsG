{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.twitter import Twitter\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "pd.set_option('max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "base_path = r'C:/Users/sshouche/Desktop/Tweet Classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"One time processing to get sector tagged data\"\"\"\n",
    "\n",
    "# drop_cols = ['Companies', 'Locations', 'Themes', 'Tweethistoryid']\n",
    "\n",
    "# for month in ['Apr', 'May', 'Jun', 'Jul', 'Aug']:\n",
    "#     dfs = pd.read_excel(base_path+r'TweetDump_last6months/TweetsDump_'+str(month)+'.xlsx', sheet_name=None)\n",
    "\n",
    "#     df = pd.concat(dfs, ignore_index=True)\n",
    "#     df = df.drop(columns=drop_cols, axis=1)\n",
    "#     df = df.dropna()\n",
    "\n",
    "#     df.to_json(base_path+r'TweetDump_last6months/Tweets_'+str(month)+'2019.json')\n",
    "\n",
    "#     del df, dfs\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"One time processing to combine sector tagged data\"\"\"\n",
    "\n",
    "# for month in ['Apr', 'May', 'Jun', 'Jul', 'Aug']:\n",
    "#     df = pd.read_json(base_path+r'TweetDump_last6months/Tweets_'+str(month)+'2019.json')\n",
    "#     if month=='Apr':\n",
    "#         df_combo = df\n",
    "#     else:\n",
    "#         df_combo = pd.concat([df_combo, df], axis=0)\n",
    "\n",
    "# del df\n",
    "# gc.collect()\n",
    "\n",
    "# df_combo = df_combo.reset_index()\n",
    "# df_combo['Sectors'] = df_combo['Sectors'].apply(lambda x: [str(y).strip() for y in str(x).split(',')])\n",
    "# df_combo.to_json(base_path+r'TweetDump_last6months/Tweets_Combo.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## DATA JOINING COMPLETE ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data and Helper Functions\"\"\"\n",
    "freqdist = nltk.FreqDist()\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "new_words = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\",\"eight\",\"nine\", \"zero\", \n",
    "             \"ten\", \"twenty\", \"thirty\", \"fourty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninty\", \n",
    "             \"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\", \"ninth\", \"tenth\",\n",
    "             \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\", \n",
    "             \"hundreds\", \"thousands\", \"millions\", \"billions\", \"trillions\",\n",
    "             \"world\", \"today\", \"would\", \"could\", \"future\", \"people\", \n",
    "             '...', 'via', 'see', 'new', 'end', 'amp', \n",
    "             'like', 'time', 'need', 'know', 'ever']\n",
    "stop_words = list(stop_words.union(new_words))\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=True, strip_handles=True, reduce_len=True)\n",
    "\n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper Functions\"\"\"\n",
    "def get_links(tweet):\n",
    "    reg_ex = r'((?:(?:https?|ftp|file):\\/\\/|www\\.|ftp\\.)*(?:[-a-zA-Z0-9@:%_\\+~.#=]{2,256})?([-a-zA-Z0-9@:%_\\+~#=]*)\\.[a-z]{2,6}\\b(?:[-a-zA-Z0-9@:%_\\+.~#?&\\/\\/=]*)*)'\n",
    "    link_regex = re.compile(reg_ex, re.DOTALL)\n",
    "    links = re.findall(link_regex, tweet)\n",
    "    link_list = []\n",
    "    for link in links:\n",
    "        link_list.append(link[0])    \n",
    "    return link_list\n",
    "\n",
    "def get_tickers(tweet):\n",
    "    ticker_regex = re.compile(r'\\$\\w*', re.DOTALL)\n",
    "    tickers = re.findall(ticker_regex, tweet)\n",
    "    ticker_list = []\n",
    "    for ticker in tickers:\n",
    "        ticker_list.append(ticker[0])    \n",
    "    return ticker_list\n",
    "\n",
    "def get_special(tweet, special_prefixes=['@', '#']):\n",
    "    words_list = []\n",
    "    for word in tweet.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] in special_prefixes:\n",
    "                words_list.append(word)\n",
    "    return words_list\n",
    "\n",
    "def strip_links(tweet):\n",
    "    # reg_ex = r'((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)'\n",
    "    # ((\\b)?((https|https|ftp|file):\\/\\/)|(www\\.|ftp\\.))?((?:[-a-zA-Z0-9@:%_\\+~.#=]{2,256}\\.)?([-a-zA-Z0-9@:%_\\+~#=]*)\\.[a-z]{2,6}\\b(?:[-a-zA-Z0-9@:%_\\+.~#?&\\/\\/=]*))*(\\b)?\n",
    "    reg_ex = r'((?:(?:https?|ftp|file):\\/\\/|www\\.|ftp\\.)*(?:[-a-zA-Z0-9@:%_\\+~.#=]{2,256})?([-a-zA-Z0-9@:%_\\+~#=]*)\\.[a-z]{2,6}\\b(?:[-a-zA-Z0-9@:%_\\+.~#?&\\/\\/=]*)*)'\n",
    "    tweet = re.sub(reg_ex, ' ', tweet)\n",
    "    # link_regex = re.compile(reg_ex, re.DOTALL)\n",
    "    # links = re.findall(link_regex, tweet)\n",
    "    # print (links)\n",
    "    # for link in links:\n",
    "    #     tweet = tweet.replace(link[0], ' ')    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "def strip_special(tweet, special_prefixes=['@', '#']):\n",
    "    for separator in string.punctuation:\n",
    "        if separator not in special_prefixes:\n",
    "            tweet = tweet.replace(separator,' ')\n",
    "    words_list = []\n",
    "    for word in tweet.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in special_prefixes:\n",
    "                words_list.append(word)\n",
    "    return ' '.join(words_list)\n",
    "\n",
    "\n",
    "def clean_tweets(tweet, keep_list=[]):\n",
    "    tweet = re.sub(r\"(Dr\\.)\", \"Doctor \", tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', ' ', tweet) # remove old style retweet text \"RT\"\n",
    "    \n",
    "    if '$' not in keep_list:\n",
    "        tweet = re.sub(r'\\$\\w*', ' ', tweet)     # remove stock market tickers like $GE\n",
    "    \n",
    "    tweet = re.sub(r'(?:\\.?)([\\w\\-_+#~!$&\\'\\.]+(?<!\\.)(@|[ ]?\\(?[ ]?(at|AT)[ ]?\\)?[ ]?)\\\n",
    "                   (?<!\\.)[\\w]+[\\w\\-\\.]*\\.[a-zA-Z-]{2,3})(?:[^\\w])', ' ', tweet) # remove emails\n",
    "    tweet = strip_links(tweet) # remove links\n",
    "    \n",
    "    if '@' not in keep_list:\n",
    "        tweet = re.sub(r'\\@[\\w.]*', ' ', tweet) #remove mentions\n",
    "    \n",
    "    if '#' not in keep_list:\n",
    "        tweet = re.sub(r'\\#[\\w.]*', ' ', tweet) # remove # from the hashtags\n",
    "        # tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    tweet = re.sub(r'([\\d]+)([\\.]{1}[\\d]*)*', ' ', tweet) # remove numbers\n",
    "    tweet = re.sub(r'([\\d]+)([\\/]{1}[\\d]+)', ' ', tweet) # remove fractions\n",
    "    tweet = re.sub(r'([\\d+])', ' ', tweet) # remove integers\n",
    "    \n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"what’s\", \"what is \", tweet)\n",
    "    tweet = re.sub(r\"\\'s\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\'ve\", \" have \", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"can not \", tweet)\n",
    "    tweet = re.sub(r\"n't\", \" not \", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"i am \", tweet)\n",
    "    tweet = re.sub(r\"\\'re\", \" are \", tweet)\n",
    "    tweet = re.sub(r\"\\'d\", \" would \", tweet)\n",
    "    tweet = re.sub(r\"\\'ll\", \" will \", tweet)\n",
    "    tweet = re.sub(r\"\\'scuse\", \" excuse \", tweet)\n",
    "    tweet = re.sub(\"\\W\", \" \", tweet) # remove single char words\n",
    "    tweet = re.sub(\"\\s+\", \" \", tweet) # remove continuous spaces\n",
    "    tweet = tweet.strip(\" \")\n",
    "    tweet = unicodedata.normalize('NFKD', tweet).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # symbol left after removing mentions, hashtags, links, emails, etc.\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'/', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    #remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    tweet = re.sub(r'_',' ', tweet)\n",
    "    tweet = tweet.strip(\" \")\n",
    "    tweet = ' '.join([x for x in tweet.split()])\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "def process_tweets(tweet, lemma_stem=True):\n",
    "    tweet = re.sub(r'\\#', '', tweet)\n",
    "    \n",
    "    tweet = [x for x in tweet.split() if len(x)>1]\n",
    "    tweet = [x for x in tweet if x not in stop_words]\n",
    "    tweet = [x for x in tweet if x not in emoticons]\n",
    "    tweet = [x for x in tweet if x not in string.punctuation]\n",
    "\n",
    "    tweet = ' '.join(tweet)\n",
    "    \n",
    "    # tokenize tweets\n",
    "    tweet_tokens = tweet_tokenizer.tokenize(tweet)\n",
    "\n",
    "    bigrams = nltk.bigrams(tweet_tokens)\n",
    "    bigrams = ['_'.join(x) for x in bigrams]\n",
    "\n",
    "    tweet_tokens = list(tweet_tokens+bigrams)\n",
    "    \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if lemma_stem:\n",
    "            lemmatize_word = lemmatizer.lemmatize(word)\n",
    "            stem_word = stemmer.stem(lemmatize_word) # stemming word\n",
    "            freqdist[stem_word]+=1\n",
    "            tweets_clean.append(stem_word)\n",
    "        else:\n",
    "            freqdist[word]+=1\n",
    "            tweets_clean.append(word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(base_path+r'TweetDump_last6months/Tweets_Combo.json')\n",
    "# df['isTagged']=0\n",
    "# df['sectorTags']=''\n",
    "# df['verticalTags']=''\n",
    "\n",
    "# df = df[pd.notnull(df['TweetFulltext'])]\n",
    "# df.to_json(base_path+r'TweetDump_last6months/Tweets_Sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Load Combined & Tagged data\"\"\"\n",
    "df_combo = pd.read_json(base_path+r'TweetDump_last6months/Tweets_Combo.json')\n",
    "# df_combo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>TweetFulltext</th>\n",
       "      <th>Sectors</th>\n",
       "      <th>CleanText</th>\n",
       "      <th>ProcessedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>874696</td>\n",
       "      <td>NealSchaffer</td>\n",
       "      <td>‘Do brands ever check on the welfare of influencers?’: YouTube stars confront mental health issues https://t.co/CIS2lnSUI1 #influencermarketing https://t.co/JfHmY8ZrVp</td>\n",
       "      <td>[Advertising, Application Software]</td>\n",
       "      <td>do brands ever check on the welfare of influencers youtube stars confront mental health issues influencermarketing</td>\n",
       "      <td>[brands, check, welfare, influencers, youtube, stars, confront, mental, health, issues, influencermarketing, brands_check, check_welfare, welfare_influencers, influencers_youtube, youtube_stars, stars_confront, confront_mental, mental_health, health_issues, issues_influencermarketing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100006</td>\n",
       "      <td>874702</td>\n",
       "      <td>NealSchaffer</td>\n",
       "      <td>Microinfluencers do endorsements right, and here are six awesome examples. https://t.co/l8RDTmWuj1 #influencermarketing https://t.co/4XAREMPD5G</td>\n",
       "      <td>[Advertising]</td>\n",
       "      <td>microinfluencers do endorsements right and here are six awesome examples influencermarketing</td>\n",
       "      <td>[microinfluencers, endorsements, right, awesome, examples, influencermarketing, microinfluencers_endorsements, endorsements_right, right_awesome, awesome_examples, examples_influencermarketing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100088</td>\n",
       "      <td>874784</td>\n",
       "      <td>NealSchaffer</td>\n",
       "      <td>Six Tips for Using Promoted Tweets for Your Ecommerce Marketing Strategy via @ryankhgb #twitter #marketing https://t.co/B4lKQFiZVs https://t.co/n1tOJf2Qjb</td>\n",
       "      <td>[Advertising, Ecommerce]</td>\n",
       "      <td>six tips for using promoted tweets for your ecommerce marketing strategy via ryankhgb twitter marketing</td>\n",
       "      <td>[tips, using, promoted, tweets, ecommerce, marketing, strategy, ryankhgb, twitter, marketing, tips_using, using_promoted, promoted_tweets, tweets_ecommerce, ecommerce_marketing, marketing_strategy, strategy_ryankhgb, ryankhgb_twitter, twitter_marketing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1003910</td>\n",
       "      <td>1778605</td>\n",
       "      <td>jetcitystar</td>\n",
       "      <td>#NasaSocial #NasaSocial #NASA747 #NASAAmes #NASAArmstrong #SOFIAtelescope #NASA #AdrianaSays #CorporateCode #USAF #Veterans #JAGNV #MoffetField #bluecube #HeartMathCoach #johnmaxwellcoach #familia #colegas #entrenamiento #mettingplanners #speaker #speakers #espanol https://t.co/m7L3RjFZ6t</td>\n",
       "      <td>[Aerospace]</td>\n",
       "      <td>nasasocial nasasocial nasa nasaames nasaarmstrong sofiatelescope nasa adrianasays corporatecode usaf veterans jagnv moffetfield bluecube heartmathcoach johnmaxwellcoach familia colegas entrenamiento mettingplanners speaker speakers espanol</td>\n",
       "      <td>[nasasocial, nasasocial, nasa, nasaames, nasaarmstrong, sofiatelescope, nasa, adrianasays, corporatecode, usaf, veterans, jagnv, moffetfield, bluecube, heartmathcoach, johnmaxwellcoach, familia, colegas, entrenamiento, mettingplanners, speaker, speakers, espanol, nasasocial_nasasocial, nasasocial_nasa, nasa_nasaames, nasaames_nasaarmstrong, nasaarmstrong_sofiatelescope, sofiatelescope_nasa, nasa_adrianasays, adrianasays_corporatecode, corporatecode_usaf, usaf_veterans, veterans_jagnv, jagnv_moffetfield, moffetfield_bluecube, bluecube_heartmathcoach, heartmathcoach_johnmaxwellcoach, johnmaxwellcoach_familia, familia_colegas, colegas_entrenamiento, entrenamiento_mettingplanners, mettingplanners_speaker, speaker_speakers, speakers_espanol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1018644</td>\n",
       "      <td>1793339</td>\n",
       "      <td>pinstripedline</td>\n",
       "      <td>RT @RoyalNavy: The last Royal Navy helicopter to fly on maritime security operations in Oman has returned to the UK after a ten-year missio…</td>\n",
       "      <td>[Maritime]</td>\n",
       "      <td>royalnavy the last royal navy helicopter to fly on maritime security operations in oman has returned to the uk after a ten year missio</td>\n",
       "      <td>[royalnavy, last, royal, navy, helicopter, fly, maritime, security, operations, oman, returned, uk, year, missio, royalnavy_last, last_royal, royal_navy, navy_helicopter, helicopter_fly, fly_maritime, maritime_security, security_operations, operations_oman, oman_returned, returned_uk, uk_year, year_missio]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index       id      ScreenName                                                                                                                                                                                                                                                                                      TweetFulltext                              Sectors                                                                                                                                                                                                                                        CleanText                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ProcessedText\n",
       "0     100000   874696   NealSchaffer    ‘Do brands ever check on the welfare of influencers?’: YouTube stars confront mental health issues https://t.co/CIS2lnSUI1 #influencermarketing https://t.co/JfHmY8ZrVp                                                                                                                            [Advertising, Application Software]  do brands ever check on the welfare of influencers youtube stars confront mental health issues influencermarketing                                                                                                                               [brands, check, welfare, influencers, youtube, stars, confront, mental, health, issues, influencermarketing, brands_check, check_welfare, welfare_influencers, influencers_youtube, youtube_stars, stars_confront, confront_mental, mental_health, health_issues, issues_influencermarketing]                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "1     100006   874702   NealSchaffer    Microinfluencers do endorsements right, and here are six awesome examples. https://t.co/l8RDTmWuj1 #influencermarketing https://t.co/4XAREMPD5G                                                                                                                                                    [Advertising]                        microinfluencers do endorsements right and here are six awesome examples influencermarketing                                                                                                                                                     [microinfluencers, endorsements, right, awesome, examples, influencermarketing, microinfluencers_endorsements, endorsements_right, right_awesome, awesome_examples, examples_influencermarketing]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "10    100088   874784   NealSchaffer    Six Tips for Using Promoted Tweets for Your Ecommerce Marketing Strategy via @ryankhgb #twitter #marketing https://t.co/B4lKQFiZVs https://t.co/n1tOJf2Qjb                                                                                                                                         [Advertising, Ecommerce]             six tips for using promoted tweets for your ecommerce marketing strategy via ryankhgb twitter marketing                                                                                                                                          [tips, using, promoted, tweets, ecommerce, marketing, strategy, ryankhgb, twitter, marketing, tips_using, using_promoted, promoted_tweets, tweets_ecommerce, ecommerce_marketing, marketing_strategy, strategy_ryankhgb, ryankhgb_twitter, twitter_marketing]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "100   1003910  1778605  jetcitystar     #NasaSocial #NasaSocial #NASA747 #NASAAmes #NASAArmstrong #SOFIAtelescope #NASA #AdrianaSays #CorporateCode #USAF #Veterans #JAGNV #MoffetField #bluecube #HeartMathCoach #johnmaxwellcoach #familia #colegas #entrenamiento #mettingplanners #speaker #speakers #espanol https://t.co/m7L3RjFZ6t  [Aerospace]                          nasasocial nasasocial nasa nasaames nasaarmstrong sofiatelescope nasa adrianasays corporatecode usaf veterans jagnv moffetfield bluecube heartmathcoach johnmaxwellcoach familia colegas entrenamiento mettingplanners speaker speakers espanol  [nasasocial, nasasocial, nasa, nasaames, nasaarmstrong, sofiatelescope, nasa, adrianasays, corporatecode, usaf, veterans, jagnv, moffetfield, bluecube, heartmathcoach, johnmaxwellcoach, familia, colegas, entrenamiento, mettingplanners, speaker, speakers, espanol, nasasocial_nasasocial, nasasocial_nasa, nasa_nasaames, nasaames_nasaarmstrong, nasaarmstrong_sofiatelescope, sofiatelescope_nasa, nasa_adrianasays, adrianasays_corporatecode, corporatecode_usaf, usaf_veterans, veterans_jagnv, jagnv_moffetfield, moffetfield_bluecube, bluecube_heartmathcoach, heartmathcoach_johnmaxwellcoach, johnmaxwellcoach_familia, familia_colegas, colegas_entrenamiento, entrenamiento_mettingplanners, mettingplanners_speaker, speaker_speakers, speakers_espanol]\n",
       "1000  1018644  1793339  pinstripedline  RT @RoyalNavy: The last Royal Navy helicopter to fly on maritime security operations in Oman has returned to the UK after a ten-year missio…                                                                                                                                                       [Maritime]                           royalnavy the last royal navy helicopter to fly on maritime security operations in oman has returned to the uk after a ten year missio                                                                                                           [royalnavy, last, royal, navy, helicopter, fly, maritime, security, operations, oman, returned, uk, year, missio, royalnavy_last, last_royal, royal_navy, navy_helicopter, helicopter_fly, fly_maritime, maritime_security, security_operations, operations_oman, oman_returned, returned_uk, uk_year, year_missio]                                                                                                                                                                                                                                                                                                                                                                                                                                                       "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Collect links, mentions, hashtags and tickers, and process Tweets\"\"\"\n",
    "gc.collect()\n",
    "\n",
    "# Get links, mentions, hashtags, tickers\n",
    "# df_combo['Links'] = df_combo['TweetFulltext'].apply(lambda x: get_links(str(x)))\n",
    "# df_combo['Mentions'] = df_combo['TweetFulltext'].apply(lambda x: [str(y).replace(':', '') for y in get_special(str(x), ['@'])])\n",
    "# df_combo['Hashtags'] = df_combo['TweetFulltext'].apply(lambda x: get_special(str(x), ['#']))\n",
    "# df_combo['Tickers'] = df_combo['TweetFulltext'].apply(lambda x: get_tickers(str(x)))\n",
    "\n",
    "# Process tweet\n",
    "df_combo['CleanText'] = df_combo['TweetFulltext'].apply(lambda x: clean_tweets(str(x), keep_list=['#', '@', '$']))\n",
    "df_combo['ProcessedText'] = df_combo['CleanText'].apply(lambda x: process_tweets(str(x), lemma_stem=False))\n",
    "\n",
    "# Process tweet\n",
    "# df_combo['ProcessedText_ls'] = df_combo['CleanText'].apply(lambda x: process_tweets(str(x)))\n",
    "\n",
    "df_combo[['index', 'id', 'ProcessedText']].to_json(base_path+r'TweetDump_last6months/Tweets_Input.json')\n",
    "df_combo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 1 elements, new values have 2 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0236ee645c2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreqdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Keyword'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Frequency'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdf_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_dist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_dist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Frequency'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34mr'TweetDump_last6months/Tweets_Keywords_no_lemma_stem.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sshouche\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5078\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5079\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5080\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5081\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5082\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sshouche\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sshouche\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    153\u001b[0m             raise ValueError(\n\u001b[0;32m    154\u001b[0m                 \u001b[1;34m'Length mismatch: Expected axis has {old} elements, new '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m                 'values have {new} elements'.format(old=old_len, new=new_len))\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 1 elements, new values have 2 elements"
     ]
    }
   ],
   "source": [
    "# \"\"\"Get keyword frequency distribution\"\"\"\n",
    "df_dist = pd.DataFrame.from_dict(freqdist, orient='index')\n",
    "df_dist = df_dist.reset_index()\n",
    "df_dist.columns = ['Keyword', 'Frequency']\n",
    "df_dist = df_dist[df_dist['Frequency']>10]\n",
    "df_dist.to_csv(base_path+r'TweetDump_last6months/Tweets_Keywords_no_lemma_stem.csv')\n",
    "df_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## DATA CLEANUP COMPLETE ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data Model\"\"\"\n",
    "import fasttext\n",
    "\n",
    "N_TAGS = 3\n",
    "\n",
    "df_combo = pd.read_json(base_path+r'TweetDump_last6months/Tweets_Input.json')\n",
    "\n",
    "df_combo['Labels'] = df_combo['Sectors'].apply(lambda x: ' '.join(['__label__' + y.replace(' ', '_') for y in x]))\n",
    "\n",
    "df_combo['InTextData'] = df_combo['Labels'] + ' ' + df_combo['ProcessedText'].apply(lambda x: ' '.join(x))\n",
    "df_combo['InCleanData'] = df_combo['Labels'] + ' ' + df_combo['CleanText']\n",
    "\n",
    "df_combo[['InTextData']].dropna().to_csv(base_path+r'InTextData.txt', header=None, index=None, sep=' ')\n",
    "df_combo[['InCleanData']].dropna().to_csv(base_path+r'InCleanData.txt', header=None, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags = pd.read_csv(base_path+r'TaggingTweets.csv')\n",
    "df_tags['Tags'] = df_tags['Tags'].apply(lambda x: x.split('`'))\n",
    "\n",
    "industry_list = ['ADAS', 'Automotive', 'Banking & Payments', 'Construction', 'Consumer', 'Foodservice', 'Insurance', \n",
    "                 'Medical', 'Mining', 'Oil & Gas', 'Packaging', 'Pharma', 'Power', 'Retail', 'Technology', 'Travel & Tourism']\n",
    "\n",
    "for ind in industry_list:\n",
    "    df_tags[df_tags['Tags'].apply(lambda x: ind in x)][['Keyword']].dropna().to_csv(base_path+ind+r'.txt', header=None, index=None, sep=' ')\n",
    "\n",
    "    \n",
    "df_combo['ProcessedTextCopy']  = df_combo['ProcessedText'].copy()\n",
    "# df_combo['TagsText'] = df_combo[['Tags']].apply(lambda x: [y for y in x if y in tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo['IndustryTags'] = [[]] * len(df_combo)\n",
    "for ind in industry_list:\n",
    "    ind_list = df_tags[df_tags['Tags'].apply(lambda x: ind in x)][['Keyword']]\n",
    "    df_combo['IndustryTags'] = df_combo['IndustryTags'].apply(lambda z: z.append(df_combo['ProcessedText'].apply(lambda x: [ind for y in x if y in ind_list])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_model = fasttext.train_supervised(base_path+r'InCleanData.txt', lr=0.05, dim=100, ws=5, epoch=5, word_ngrams=2, loss='softmax', verbose=0)\n",
    "clean_model.save_model(base_path+r'clean_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_model = fasttext.train_supervised(base_path+r'InTextData.txt', lr=0.05, dim=100, ws=5, epoch=5, loss='softmax', verbose=0)\n",
    "# text_model.save_model(base_path+r'text_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashtag_model = fasttext.train_supervised(base_path+r'InHashtagData.txt', lr=0.05, dim=100, ws=5, epoch=5, loss='softmax', verbose=0)\n",
    "# hashtag_model.save_model(base_path+r'hashtag_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_clean = '#AI & #machinelearning let us discover solutions in a faster and more agile way than ever before:'\n",
    "in_clean = clean_tweets(str(in_clean), keep_list=['#'])\n",
    "print (in_clean)\n",
    "clean_result = clean_model.predict([in_clean], N_TAGS)\n",
    "print (clean_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combo.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>TweetFulltext</th>\n",
       "      <th>Sectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>874696</td>\n",
       "      <td>NealSchaffer</td>\n",
       "      <td>‘Do brands ever check on the welfare of influe...</td>\n",
       "      <td>[Advertising, Application Software]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100006</td>\n",
       "      <td>874702</td>\n",
       "      <td>NealSchaffer</td>\n",
       "      <td>Microinfluencers do endorsements right, and he...</td>\n",
       "      <td>[Advertising]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100088</td>\n",
       "      <td>874784</td>\n",
       "      <td>NealSchaffer</td>\n",
       "      <td>Six Tips for Using Promoted Tweets for Your Ec...</td>\n",
       "      <td>[Advertising, Ecommerce]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1003910</td>\n",
       "      <td>1778605</td>\n",
       "      <td>jetcitystar</td>\n",
       "      <td>#NasaSocial #NasaSocial #NASA747 #NASAAmes #NA...</td>\n",
       "      <td>[Aerospace]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1018644</td>\n",
       "      <td>1793339</td>\n",
       "      <td>pinstripedline</td>\n",
       "      <td>RT @RoyalNavy: The last Royal Navy helicopter ...</td>\n",
       "      <td>[Maritime]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>128748</td>\n",
       "      <td>903444</td>\n",
       "      <td>evankirstel</td>\n",
       "      <td>Are the Marines getting night-vision drones? h...</td>\n",
       "      <td>[Aerospace, Maritime, Weaponry]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>1543159</td>\n",
       "      <td>1543158</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @RathfinnyEstate: Georgia heads up our wine...</td>\n",
       "      <td>[Intermediaries]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>1543161</td>\n",
       "      <td>1543160</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @fabienlaine: Office for the afternoon 🤩🍷ye...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>1543166</td>\n",
       "      <td>1543165</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @wineworldnews: This is the #wine you shoul...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>1543168</td>\n",
       "      <td>1543167</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @DemiCassiani: Take Life One Sip At A Time ...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>1543173</td>\n",
       "      <td>1543172</td>\n",
       "      <td>DrSheilaSahni</td>\n",
       "      <td>RT @Drroxmehran: @ajaykirtane It’s just hidden...</td>\n",
       "      <td>[Cardiovascular]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>1543177</td>\n",
       "      <td>1543176</td>\n",
       "      <td>DrSheilaSahni</td>\n",
       "      <td>RT @Rahul2282Sharma: Privileged to speak at #S...</td>\n",
       "      <td>[Cardiovascular]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100006</th>\n",
       "      <td>1543192</td>\n",
       "      <td>1543191</td>\n",
       "      <td>DrSheilaSahni</td>\n",
       "      <td>Don’t miss #SCAI2019 #SCAIWIN TONIGHT 645pm Wo...</td>\n",
       "      <td>[Cardiovascular]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100007</th>\n",
       "      <td>1543197</td>\n",
       "      <td>1543196</td>\n",
       "      <td>DrSheilaSahni</td>\n",
       "      <td>RT @onco_cardiology: Looking forward for #SCAI...</td>\n",
       "      <td>[Metabolic Disorders]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100008</th>\n",
       "      <td>1543212</td>\n",
       "      <td>1543211</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @wineworldnews: and you? 🧐🍷We Asked 15 Somm...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100009</th>\n",
       "      <td>1543213</td>\n",
       "      <td>1543212</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @BibendaAssisi: We are proud to inform you ...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>1287560</td>\n",
       "      <td>2062255</td>\n",
       "      <td>SamRo</td>\n",
       "      <td>Bank accounts can’t talk</td>\n",
       "      <td>[Retail Banking]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100010</th>\n",
       "      <td>1543217</td>\n",
       "      <td>1543216</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @wineworldnews: The World's Most Wanted Zin...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100011</th>\n",
       "      <td>1543222</td>\n",
       "      <td>1543221</td>\n",
       "      <td>DrSheilaSahni</td>\n",
       "      <td>RT @DrMarthaGulati: I love that the face of me...</td>\n",
       "      <td>[Cardiovascular]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100012</th>\n",
       "      <td>1543223</td>\n",
       "      <td>1543222</td>\n",
       "      <td>DrSheilaSahni</td>\n",
       "      <td>@TheNarulaSeries @JWMoses @JAG24851 @J_M_McCab...</td>\n",
       "      <td>[Cardiovascular]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100013</th>\n",
       "      <td>1543234</td>\n",
       "      <td>1543233</td>\n",
       "      <td>DrSheilaSahni</td>\n",
       "      <td>RT @SCAI: SCAI Releases Multi-Society Endorsed...</td>\n",
       "      <td>[Cardiovascular]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100014</th>\n",
       "      <td>1543255</td>\n",
       "      <td>1543254</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @LisaRivera2207: Duomos, chocolate &amp;amp; a ...</td>\n",
       "      <td>[Food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100015</th>\n",
       "      <td>1543256</td>\n",
       "      <td>1543255</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @GailBenzler: You guys! I can’t stay away f...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100016</th>\n",
       "      <td>1543269</td>\n",
       "      <td>1543268</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>@Ryanair \\nCan you please tell me what is happ...</td>\n",
       "      <td>[Airlines]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100017</th>\n",
       "      <td>1543272</td>\n",
       "      <td>1543271</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @wineworldnews: What is Prosecco? Infograph...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100018</th>\n",
       "      <td>1543281</td>\n",
       "      <td>1543280</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @wineworldnews: Thirst is what Bad 😁🍷🍾🍷😁👍🍷#...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100019</th>\n",
       "      <td>1543286</td>\n",
       "      <td>1543285</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @suziday123: Happy #WineWednesday everyone ...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>1287581</td>\n",
       "      <td>2062276</td>\n",
       "      <td>jeffsheehan</td>\n",
       "      <td>The Future Of Content Marketing Infused With A...</td>\n",
       "      <td>[Advertising]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100020</th>\n",
       "      <td>1543309</td>\n",
       "      <td>1543308</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @TheWineKiwi: Sometimes we take a chance on...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100021</th>\n",
       "      <td>1543317</td>\n",
       "      <td>1543316</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @JMiquelWine: Walk around Banyuls... the la...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99972</th>\n",
       "      <td>1542652</td>\n",
       "      <td>1542651</td>\n",
       "      <td>InfoSecSherpa</td>\n",
       "      <td>Exactly. But the big ass Costco bags make more...</td>\n",
       "      <td>[Department Stores]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99973</th>\n",
       "      <td>1542662</td>\n",
       "      <td>1542661</td>\n",
       "      <td>InfoSecSherpa</td>\n",
       "      <td>Just get a giant bag of fresh spinach from Cos...</td>\n",
       "      <td>[Department Stores, Food, Food &amp; Grocery, Heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>1542695</td>\n",
       "      <td>1542694</td>\n",
       "      <td>InfoSecSherpa</td>\n",
       "      <td>RT @Dreamchaser_NFL: I just want to get my foo...</td>\n",
       "      <td>[Security Software]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>154281</td>\n",
       "      <td>1754281</td>\n",
       "      <td>TamzinSwann</td>\n",
       "      <td>What is the best hotel to stay at in Rivera Ma...</td>\n",
       "      <td>[Lodging]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>1542909</td>\n",
       "      <td>1542908</td>\n",
       "      <td>TheMehulPatel</td>\n",
       "      <td>\"Commerce Dept. creates a general license, whi...</td>\n",
       "      <td>[Telecom Operators]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>1542966</td>\n",
       "      <td>1542965</td>\n",
       "      <td>mvollmer1</td>\n",
       "      <td>RT @WiproDigital: How to Design a Successful #...</td>\n",
       "      <td>[IT Services]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>1542985</td>\n",
       "      <td>1542984</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>What's the biggest bottle of #wine you've had?...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>1542992</td>\n",
       "      <td>1542991</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @andrij_wine: Loving this @TioPepeFino #EnR...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>128739</td>\n",
       "      <td>903435</td>\n",
       "      <td>psb_dc</td>\n",
       "      <td>Social media alert: How to remain compliant \\n...</td>\n",
       "      <td>[Wealth management]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>1542998</td>\n",
       "      <td>1542997</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @DivaVinophile: #Montreal visit is not comp...</td>\n",
       "      <td>[Alcoholic Beverages, Food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>1543000</td>\n",
       "      <td>1542999</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @CaththeWineLady: I’ve decided to redecorat...</td>\n",
       "      <td>[Alcoholic Beverages, Home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>1543005</td>\n",
       "      <td>1543004</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @DemiCassiani: This #pizza is making me hun...</td>\n",
       "      <td>[Alcoholic Beverages, Food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>1543014</td>\n",
       "      <td>1543013</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>James I have to say that it's a delicious wine...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>1543035</td>\n",
       "      <td>1543034</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @GailBenzler: I'm excited! Old World vs New...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>1543046</td>\n",
       "      <td>1543045</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @JMiquelWine: Open a #Wine Bottle with a Li...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>1543055</td>\n",
       "      <td>1543054</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @wineworldnews: Today without alcohol. Mang...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>1543057</td>\n",
       "      <td>1543056</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @thewinetattoo: Last post of amazing #wine ...</td>\n",
       "      <td>[Alcoholic Beverages, Food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>1543060</td>\n",
       "      <td>1543059</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @suziday123: Hi #WiningHourChat friends 🙋‍♀...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99989</th>\n",
       "      <td>1543061</td>\n",
       "      <td>1543060</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @DemiCassiani: It’s Never Too Cold for #Cha...</td>\n",
       "      <td>[Alcoholic Beverages, Food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1287395</td>\n",
       "      <td>2062090</td>\n",
       "      <td>tomemrich</td>\n",
       "      <td>“Maybe we have to have a different approach to...</td>\n",
       "      <td>[Telecom Operators]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99990</th>\n",
       "      <td>1543072</td>\n",
       "      <td>1543071</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @DemiCassiani: #Lunch Box\\n.\\n.\\n#wine #foo...</td>\n",
       "      <td>[Alcoholic Beverages, Food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>1543073</td>\n",
       "      <td>1543072</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @wineworldnews: What some people are worrie...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>1543075</td>\n",
       "      <td>1543074</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>It's great too, interesting wine, 🍷🍷🍷🇬🇧🇬🇧🇬🇧🇬🇧🍷🍷</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99993</th>\n",
       "      <td>1543081</td>\n",
       "      <td>1543080</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>@DivaVinophile @DaneburyFizz @RichLJames @jimo...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>1543091</td>\n",
       "      <td>1543090</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @wineworldnews: The Top-Rated Sweet Red Win...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1543096</td>\n",
       "      <td>1543095</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @DemiCassiani: #Yummy 🤤\\n.\\n.\\n#wine #food ...</td>\n",
       "      <td>[Alcoholic Beverages, Food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1543142</td>\n",
       "      <td>1543141</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @TweetaDean: English sparkling wine in a ca...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>1543144</td>\n",
       "      <td>1543143</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @andreacarozzo4: About my birthday.. \\n@Cot...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1543146</td>\n",
       "      <td>1543145</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>Love the wine, 🍷😍🍷</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1543148</td>\n",
       "      <td>1543147</td>\n",
       "      <td>pietrosd</td>\n",
       "      <td>RT @wineworldnews: Interesting 🧐🍷🤔🍷 Can You Dr...</td>\n",
       "      <td>[Alcoholic Beverages]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>536443 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          index       id      ScreenName  \\\n",
       "0        100000   874696    NealSchaffer   \n",
       "1        100006   874702    NealSchaffer   \n",
       "10       100088   874784    NealSchaffer   \n",
       "100     1003910  1778605     jetcitystar   \n",
       "1000    1018644  1793339  pinstripedline   \n",
       "10000    128748   903444     evankirstel   \n",
       "100000  1543159  1543158        pietrosd   \n",
       "100001  1543161  1543160        pietrosd   \n",
       "100002  1543166  1543165        pietrosd   \n",
       "100003  1543168  1543167        pietrosd   \n",
       "100004  1543173  1543172   DrSheilaSahni   \n",
       "100005  1543177  1543176   DrSheilaSahni   \n",
       "100006  1543192  1543191   DrSheilaSahni   \n",
       "100007  1543197  1543196   DrSheilaSahni   \n",
       "100008  1543212  1543211        pietrosd   \n",
       "100009  1543213  1543212        pietrosd   \n",
       "10001   1287560  2062255           SamRo   \n",
       "100010  1543217  1543216        pietrosd   \n",
       "100011  1543222  1543221   DrSheilaSahni   \n",
       "100012  1543223  1543222   DrSheilaSahni   \n",
       "100013  1543234  1543233   DrSheilaSahni   \n",
       "100014  1543255  1543254        pietrosd   \n",
       "100015  1543256  1543255        pietrosd   \n",
       "100016  1543269  1543268        pietrosd   \n",
       "100017  1543272  1543271        pietrosd   \n",
       "100018  1543281  1543280        pietrosd   \n",
       "100019  1543286  1543285        pietrosd   \n",
       "10002   1287581  2062276     jeffsheehan   \n",
       "100020  1543309  1543308        pietrosd   \n",
       "100021  1543317  1543316        pietrosd   \n",
       "...         ...      ...             ...   \n",
       "99972   1542652  1542651   InfoSecSherpa   \n",
       "99973   1542662  1542661   InfoSecSherpa   \n",
       "99974   1542695  1542694   InfoSecSherpa   \n",
       "99975    154281  1754281     TamzinSwann   \n",
       "99976   1542909  1542908   TheMehulPatel   \n",
       "99977   1542966  1542965       mvollmer1   \n",
       "99978   1542985  1542984        pietrosd   \n",
       "99979   1542992  1542991        pietrosd   \n",
       "9998     128739   903435          psb_dc   \n",
       "99980   1542998  1542997        pietrosd   \n",
       "99981   1543000  1542999        pietrosd   \n",
       "99982   1543005  1543004        pietrosd   \n",
       "99983   1543014  1543013        pietrosd   \n",
       "99984   1543035  1543034        pietrosd   \n",
       "99985   1543046  1543045        pietrosd   \n",
       "99986   1543055  1543054        pietrosd   \n",
       "99987   1543057  1543056        pietrosd   \n",
       "99988   1543060  1543059        pietrosd   \n",
       "99989   1543061  1543060        pietrosd   \n",
       "9999    1287395  2062090       tomemrich   \n",
       "99990   1543072  1543071        pietrosd   \n",
       "99991   1543073  1543072        pietrosd   \n",
       "99992   1543075  1543074        pietrosd   \n",
       "99993   1543081  1543080        pietrosd   \n",
       "99994   1543091  1543090        pietrosd   \n",
       "99995   1543096  1543095        pietrosd   \n",
       "99996   1543142  1543141        pietrosd   \n",
       "99997   1543144  1543143        pietrosd   \n",
       "99998   1543146  1543145        pietrosd   \n",
       "99999   1543148  1543147        pietrosd   \n",
       "\n",
       "                                            TweetFulltext  \\\n",
       "0       ‘Do brands ever check on the welfare of influe...   \n",
       "1       Microinfluencers do endorsements right, and he...   \n",
       "10      Six Tips for Using Promoted Tweets for Your Ec...   \n",
       "100     #NasaSocial #NasaSocial #NASA747 #NASAAmes #NA...   \n",
       "1000    RT @RoyalNavy: The last Royal Navy helicopter ...   \n",
       "10000   Are the Marines getting night-vision drones? h...   \n",
       "100000  RT @RathfinnyEstate: Georgia heads up our wine...   \n",
       "100001  RT @fabienlaine: Office for the afternoon 🤩🍷ye...   \n",
       "100002  RT @wineworldnews: This is the #wine you shoul...   \n",
       "100003  RT @DemiCassiani: Take Life One Sip At A Time ...   \n",
       "100004  RT @Drroxmehran: @ajaykirtane It’s just hidden...   \n",
       "100005  RT @Rahul2282Sharma: Privileged to speak at #S...   \n",
       "100006  Don’t miss #SCAI2019 #SCAIWIN TONIGHT 645pm Wo...   \n",
       "100007  RT @onco_cardiology: Looking forward for #SCAI...   \n",
       "100008  RT @wineworldnews: and you? 🧐🍷We Asked 15 Somm...   \n",
       "100009  RT @BibendaAssisi: We are proud to inform you ...   \n",
       "10001                            Bank accounts can’t talk   \n",
       "100010  RT @wineworldnews: The World's Most Wanted Zin...   \n",
       "100011  RT @DrMarthaGulati: I love that the face of me...   \n",
       "100012  @TheNarulaSeries @JWMoses @JAG24851 @J_M_McCab...   \n",
       "100013  RT @SCAI: SCAI Releases Multi-Society Endorsed...   \n",
       "100014  RT @LisaRivera2207: Duomos, chocolate &amp; a ...   \n",
       "100015  RT @GailBenzler: You guys! I can’t stay away f...   \n",
       "100016  @Ryanair \\nCan you please tell me what is happ...   \n",
       "100017  RT @wineworldnews: What is Prosecco? Infograph...   \n",
       "100018  RT @wineworldnews: Thirst is what Bad 😁🍷🍾🍷😁👍🍷#...   \n",
       "100019  RT @suziday123: Happy #WineWednesday everyone ...   \n",
       "10002   The Future Of Content Marketing Infused With A...   \n",
       "100020  RT @TheWineKiwi: Sometimes we take a chance on...   \n",
       "100021  RT @JMiquelWine: Walk around Banyuls... the la...   \n",
       "...                                                   ...   \n",
       "99972   Exactly. But the big ass Costco bags make more...   \n",
       "99973   Just get a giant bag of fresh spinach from Cos...   \n",
       "99974   RT @Dreamchaser_NFL: I just want to get my foo...   \n",
       "99975   What is the best hotel to stay at in Rivera Ma...   \n",
       "99976   \"Commerce Dept. creates a general license, whi...   \n",
       "99977   RT @WiproDigital: How to Design a Successful #...   \n",
       "99978   What's the biggest bottle of #wine you've had?...   \n",
       "99979   RT @andrij_wine: Loving this @TioPepeFino #EnR...   \n",
       "9998    Social media alert: How to remain compliant \\n...   \n",
       "99980   RT @DivaVinophile: #Montreal visit is not comp...   \n",
       "99981   RT @CaththeWineLady: I’ve decided to redecorat...   \n",
       "99982   RT @DemiCassiani: This #pizza is making me hun...   \n",
       "99983   James I have to say that it's a delicious wine...   \n",
       "99984   RT @GailBenzler: I'm excited! Old World vs New...   \n",
       "99985   RT @JMiquelWine: Open a #Wine Bottle with a Li...   \n",
       "99986   RT @wineworldnews: Today without alcohol. Mang...   \n",
       "99987   RT @thewinetattoo: Last post of amazing #wine ...   \n",
       "99988   RT @suziday123: Hi #WiningHourChat friends 🙋‍♀...   \n",
       "99989   RT @DemiCassiani: It’s Never Too Cold for #Cha...   \n",
       "9999    “Maybe we have to have a different approach to...   \n",
       "99990   RT @DemiCassiani: #Lunch Box\\n.\\n.\\n#wine #foo...   \n",
       "99991   RT @wineworldnews: What some people are worrie...   \n",
       "99992     It's great too, interesting wine, 🍷🍷🍷🇬🇧🇬🇧🇬🇧🇬🇧🍷🍷   \n",
       "99993   @DivaVinophile @DaneburyFizz @RichLJames @jimo...   \n",
       "99994   RT @wineworldnews: The Top-Rated Sweet Red Win...   \n",
       "99995   RT @DemiCassiani: #Yummy 🤤\\n.\\n.\\n#wine #food ...   \n",
       "99996   RT @TweetaDean: English sparkling wine in a ca...   \n",
       "99997   RT @andreacarozzo4: About my birthday.. \\n@Cot...   \n",
       "99998                                  Love the wine, 🍷😍🍷   \n",
       "99999   RT @wineworldnews: Interesting 🧐🍷🤔🍷 Can You Dr...   \n",
       "\n",
       "                                                  Sectors  \n",
       "0                     [Advertising, Application Software]  \n",
       "1                                           [Advertising]  \n",
       "10                               [Advertising, Ecommerce]  \n",
       "100                                           [Aerospace]  \n",
       "1000                                           [Maritime]  \n",
       "10000                     [Aerospace, Maritime, Weaponry]  \n",
       "100000                                   [Intermediaries]  \n",
       "100001                              [Alcoholic Beverages]  \n",
       "100002                              [Alcoholic Beverages]  \n",
       "100003                              [Alcoholic Beverages]  \n",
       "100004                                   [Cardiovascular]  \n",
       "100005                                   [Cardiovascular]  \n",
       "100006                                   [Cardiovascular]  \n",
       "100007                              [Metabolic Disorders]  \n",
       "100008                              [Alcoholic Beverages]  \n",
       "100009                              [Alcoholic Beverages]  \n",
       "10001                                    [Retail Banking]  \n",
       "100010                              [Alcoholic Beverages]  \n",
       "100011                                   [Cardiovascular]  \n",
       "100012                                   [Cardiovascular]  \n",
       "100013                                   [Cardiovascular]  \n",
       "100014                                             [Food]  \n",
       "100015                              [Alcoholic Beverages]  \n",
       "100016                                         [Airlines]  \n",
       "100017                              [Alcoholic Beverages]  \n",
       "100018                              [Alcoholic Beverages]  \n",
       "100019                              [Alcoholic Beverages]  \n",
       "10002                                       [Advertising]  \n",
       "100020                              [Alcoholic Beverages]  \n",
       "100021                              [Alcoholic Beverages]  \n",
       "...                                                   ...  \n",
       "99972                                 [Department Stores]  \n",
       "99973   [Department Stores, Food, Food & Grocery, Heal...  \n",
       "99974                                 [Security Software]  \n",
       "99975                                           [Lodging]  \n",
       "99976                                 [Telecom Operators]  \n",
       "99977                                       [IT Services]  \n",
       "99978                               [Alcoholic Beverages]  \n",
       "99979                               [Alcoholic Beverages]  \n",
       "9998                                  [Wealth management]  \n",
       "99980                         [Alcoholic Beverages, Food]  \n",
       "99981                         [Alcoholic Beverages, Home]  \n",
       "99982                         [Alcoholic Beverages, Food]  \n",
       "99983                               [Alcoholic Beverages]  \n",
       "99984                               [Alcoholic Beverages]  \n",
       "99985                               [Alcoholic Beverages]  \n",
       "99986                               [Alcoholic Beverages]  \n",
       "99987                         [Alcoholic Beverages, Food]  \n",
       "99988                               [Alcoholic Beverages]  \n",
       "99989                         [Alcoholic Beverages, Food]  \n",
       "9999                                  [Telecom Operators]  \n",
       "99990                         [Alcoholic Beverages, Food]  \n",
       "99991                               [Alcoholic Beverages]  \n",
       "99992                               [Alcoholic Beverages]  \n",
       "99993                               [Alcoholic Beverages]  \n",
       "99994                               [Alcoholic Beverages]  \n",
       "99995                         [Alcoholic Beverages, Food]  \n",
       "99996                               [Alcoholic Beverages]  \n",
       "99997                               [Alcoholic Beverages]  \n",
       "99998                               [Alcoholic Beverages]  \n",
       "99999                               [Alcoholic Beverages]  \n",
       "\n",
       "[536443 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json('Tweets_Combo.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Sectors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Sectors'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c4e9dda0abb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# df_combo_2 = df_combo.join(pd.DataFrame(mlb.fit_transform(df_combo['Sectors']), columns=mlb.classes_, index=df_combo.index))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_combo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Sectors'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_combo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CleanText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mpop\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[0mmonkey\u001b[0m        \u001b[0mNaN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m         \"\"\"\n\u001b[1;32m--> 809\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Sectors'"
     ]
    }
   ],
   "source": [
    "\"\"\"Model training\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "df_combo = pd.read_json(r'Tweets_Input.json')\n",
    "# df_combo_2 = df_combo.join(pd.DataFrame(mlb.fit_transform(df_combo['Sectors']), columns=mlb.classes_, index=df_combo.index))\n",
    "\n",
    "y = mlb.fit_transform(df_combo.pop('Sectors'))\n",
    "X = df_combo['CleanText']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "clf = OneVsRestClassifier(MultinomialNB())\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "print (accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df['Sectors'].shape[0])\n",
    "print (df['Sectors'].isna().sum())\n",
    "print (df['Sectors'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x=list(df['Themes'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Locations','Companies','Sectors','Tweethistoryid','ScreenName'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Themes'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# le.fit(df['Themes'])\n",
    "# le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "# print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=['3D Printing', '5G', 'Artificial Intelligence', 'Cloud', 'Digital Assistants', 'E-commerce', 'Internet of Things', 'Mobile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance\n",
    "ADS\n",
    "Automotive\n",
    "Banking_Payments\n",
    "Construction\n",
    "Consumer\n",
    "Food_Service\n",
    "Medical_Devices\n",
    "Mining\n",
    "Packaging\n",
    "Pharma\n",
    "Power\n",
    "Retail\n",
    "Technology\n",
    "Travel_Tourism\n",
    "Upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sector']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['TweetFulltext'].str.contains('|'.join(Upstream))==True,'Sector']='Upstream'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Apr_Training_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.read_excel(r'C:\\Users\\kasandeep\\Desktop\\Projects\\Twitter_Sector_Classification\\Sector View Keywords\\Consolidated IC and Sector Screeners & Keywords.xlsx',sheet_name=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(df3['Screener/Filter word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[x.strip(\"#|@|'|\\xa0|\\u200f\") for x in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[x.replace(\" \",\"\") for x in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=set([x.replace(\"-\",\"\") for x in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=set([x.replace(\"'\",\"\") for x in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=set([x.lower() for x in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=[]\n",
    "# from nltk.stem import PorterStemmer\n",
    "# stemmer= PorterStemmer()\n",
    "# for i in x:\n",
    "#     y.append(stemmer.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=[]\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for i in x:\n",
    "    z.append(wordnet_lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\kasandeep\\Desktop\\Projects\\Twitter_Sector_Classification\\Sector View Keywords\\Travel_Tourism_words.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.write('\\n'.join(str(line) for line in set(Travel_Tourism)))\n",
    "    myfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance\n",
    "ADS\n",
    "Automotive\n",
    "Banking_Payments\n",
    "Construction\n",
    "Consumer\n",
    "Food_Service\n",
    "Medical_Devices\n",
    "Mining\n",
    "Packaging\n",
    "Pharma\n",
    "Power\n",
    "Retail\n",
    "Technology\n",
    "Travel_Tourism\n",
    "Upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r\"C:\\Users\\kasandeep\\Desktop\\Projects\\Twitter_Sector_Classification\\Sector View Keywords\\Upstream_words.txt\", \"r\",encoding=\"utf8\")\n",
    "z=Upstream\n",
    "for x in f:\n",
    "      z.append(x.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Upstream=[]\n",
    "for i in set(z):\n",
    "    if sum(i in s for s in df['TweetFulltext'])>50:\n",
    "        Upstream.append(i)\n",
    "        print(i,sum(i in s for s in df['TweetFulltext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r\"C:\\Users\\kasandeep\\Desktop\\Projects\\Twitter_Sector_Classification\\Sector View Keywords\\Packaging.txt\", \"r\",encoding=\"utf8\")\n",
    "z=[]\n",
    "for x in f:\n",
    "      z.append(x.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Consumer=[]\n",
    "for i in set(z):\n",
    "    if sum(i in s for s in df['TweetFulltext'])>50:\n",
    "        Consumer.append(i)\n",
    "        print(i,sum(i in s for s in df['TweetFulltext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "from gensim.models import CoherenceModel\n",
    "#np.random.seed(2018)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = df['TweetFulltext'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(bow_corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, dictionary=dictionary, texts=processed_docs, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize t# Visualize the topics\n",
    "# import pyLDAvis.gensim\n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        #model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model=gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=bow_corpus, texts=processed_docs, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
